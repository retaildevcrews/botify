{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec6048-44e4-4118-b16a-9c4c9cc78a3b",
   "metadata": {},
   "source": [
    "# How to deal with json documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281ac79-47cd-49d4-bdd4-7f5c173a947d",
   "metadata": {},
   "source": [
    "Load data from JSON formatted document into the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../apps/credentials.env\")\n",
    "\n",
    "\n",
    "def text_to_base64(text):\n",
    "    # Convert text to bytes using UTF-8 encoding\n",
    "    bytes_data = text.encode(\"utf-8\")\n",
    "\n",
    "    # Perform Base64 encoding\n",
    "    base64_encoded = base64.b64encode(bytes_data)\n",
    "\n",
    "    # Convert the result back to a UTF-8 string representation\n",
    "    base64_text = base64_encoded.decode(\"utf-8\")\n",
    "\n",
    "    return base64_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331692ba-b68e-4b99-9bae-5057da9a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = AzureOpenAIEmbeddings(deployment=os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"], chunk_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
   "metadata": {},
   "source": [
    "## Create Vector-based index\n",
    "\n",
    "\n",
    "Now that we have the content of the book's chunks (each page of each book) in the dictionary `book_pages_map`, let's create the Vector index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
    "print(\"Index name: \", index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Azure Search Vector-based Index\n",
    "# Setup the Payloads header\n",
    "headers = {\"Content-Type\": \"application/json\", \"api-key\": os.environ[\"AZURE_SEARCH_KEY\"]}\n",
    "params = {\"api-version\": os.environ[\"AZURE_SEARCH_API_VERSION\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d63e68-69a5-4b3b-8eb0-86da02cb7230",
   "metadata": {},
   "source": [
    "REST API version 2024-05-01-preview supports external and internal vectorization. This Notebook assumes an external vectorization strategy. This API also supports:\n",
    "    \n",
    "- vectorSearch algorithms, hnsw and exhaustiveKnn nearest neighbors, with parameters for indexing and scoring.\n",
    "- vectorProfiles for multiple combinations of algorithm configurations.\n",
    "\n",
    "Vector search algorithms include **exhaustive k-nearest neighbors (KNN)** and **Hierarchical Navigable Small World (HNSW)**. Exhaustive KNN performs a brute-force search that scans the entire vector space. HNSW performs an approximate nearest neighbor (ANN) search. While KNN provides exact nearest neighbor search results with high accuracy, its computational cost and poor scalability make it impractical for large datasets or real-time applications. HNSW, on the other hand, offers a highly efficient and scalable solution for nearest neighbor searches by finding approximate nearest neighbors quickly, making it more suitable for large-scale and high-dimensional data applications.\n",
    "\n",
    "\n",
    "check [HERE](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-create-index?tabs=config-2024-05-01-Preview%2Crest-2024-07-01%2Cpush%2Cportal-check-index) for the details of the vector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithms\": [{\"name\": \"myalgo\", \"kind\": \"hnsw\"}],\n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"openai\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\": {\n",
    "                    \"resourceUri\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                    \"apiKey\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "                    \"deploymentId\": os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"],\n",
    "                    \"modelName\": os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [{\"name\": \"myprofile\", \"algorithm\": \"myalgo\", \"vectorizer\": \"openai\"}],\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\"fieldName\": \"title\"},\n",
    "                    \"prioritizedContentFields\": [{\"fieldName\": \"chunk\"}],\n",
    "                    \"prioritizedKeywordsFields\": [],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"id\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"key\": \"true\",\n",
    "            \"analyzer\": \"keyword\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ParentKey\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"facetable\": \"false\",\n",
    "            \"filterable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"title\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"facetable\": \"false\",\n",
    "            \"filterable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"name\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"location\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"chunk\",\n",
    "            \"type\": \"Edm.String\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"facetable\": \"false\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"type\": \"Collection(Edm.Single)\",\n",
    "            \"dimensions\": 1536,  # IMPORTANT: Make sure these dimmensions match your embedding model name\n",
    "            \"vectorSearchProfile\": \"myprofile\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"facetable\": \"false\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "url = f\"{os.environ['AZURE_SEARCH_ENDPOINT']}/indexes/{index_name}\"\n",
    "print(url)\n",
    "r = requests.put(url, data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465868b0",
   "metadata": {},
   "source": [
    "# Create the indexer skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skillset_name = index_name + \"skillset\"\n",
    "print(\"Skillset name: \", skillset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a skillset\n",
    "skillset_payload = {\n",
    "    \"name\": skillset_name,\n",
    "    \"description\": \"e2e Skillset for RAG - Files\",\n",
    "    \"skills\": [\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "            \"description\": \"Extract text (plain and structured) from image.\",\n",
    "            \"context\": \"/document/normalized_images/*\",\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"detectOrientation\": True,\n",
    "            \"inputs\": [{\"name\": \"image\", \"source\": \"/document/normalized_images/*\"}],\n",
    "            \"outputs\": [{\"name\": \"text\", \"targetName\": \"images_text\"}],\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "            \"description\": \"Create merged_text, which includes all the textual representation of each image inserted at the right location in the content field. This is useful for PDF and other file formats that supported embedded images.\",\n",
    "            \"context\": \"/document\",\n",
    "            \"insertPreTag\": \" \",\n",
    "            \"insertPostTag\": \" \",\n",
    "            \"inputs\": [\n",
    "                {\"name\": \"text\", \"source\": \"/document/content\"},\n",
    "                {\"name\": \"itemsToInsert\", \"source\": \"/document/normalized_images/*/images_text\"},\n",
    "                {\"name\": \"offsets\", \"source\": \"/document/normalized_images/*/contentOffset\"},\n",
    "            ],\n",
    "            \"outputs\": [{\"name\": \"mergedText\", \"targetName\": \"merged_text\"}],\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "            \"context\": \"/document\",\n",
    "            \"textSplitMode\": \"pages\",  # although it says \"pages\" it actally means chunks, not actual pages\n",
    "            \"maximumPageLength\": 5000,  # 5000 characters is default and a good choice\n",
    "            \"pageOverlapLength\": 750,  # 15% overlap among chunks\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"inputs\": [{\"name\": \"text\", \"source\": \"/document/merged_text\"}],\n",
    "            \"outputs\": [{\"name\": \"textItems\", \"targetName\": \"chunks\"}],\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "            \"description\": \"Azure OpenAI Embedding Skill\",\n",
    "            \"context\": \"/document/chunks/*\",\n",
    "            \"resourceUri\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            \"apiKey\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            \"deploymentId\": os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"],\n",
    "            \"modelName\": os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"],\n",
    "            \"inputs\": [{\"name\": \"text\", \"source\": \"/document/chunks/*\"}],\n",
    "            \"outputs\": [{\"name\": \"embedding\", \"targetName\": \"vector\"}],\n",
    "        },\n",
    "    ],\n",
    "    \"indexProjections\": {\n",
    "        \"selectors\": [\n",
    "            {\n",
    "                \"targetIndexName\": index_name,\n",
    "                \"parentKeyFieldName\": \"ParentKey\",\n",
    "                \"sourceContext\": \"/document/chunks/*\",\n",
    "                \"mappings\": [\n",
    "                    {\"name\": \"title\", \"source\": \"/document/title\"},\n",
    "                    {\"name\": \"name\", \"source\": \"/document/name\"},\n",
    "                    {\"name\": \"location\", \"source\": \"/document/location\"},\n",
    "                    {\"name\": \"chunk\", \"source\": \"/document/chunks/*\"},\n",
    "                    {\"name\": \"chunkVector\", \"source\": \"/document/chunks/*/vector\"},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\"projectionMode\": \"skipIndexingParentDocuments\"},\n",
    "    },\n",
    "    \"cognitiveServices\": {\n",
    "        \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
    "        \"description\": os.environ[\"COG_SERVICES_NAME\"],\n",
    "        \"key\": os.environ[\"COG_SERVICES_KEY\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "r = requests.put(\n",
    "    os.environ[\"AZURE_SEARCH_ENDPOINT\"] + \"/skillsets/\" + skillset_name,\n",
    "    data=json.dumps(skillset_payload),\n",
    "    headers=headers,\n",
    "    params=params,\n",
    ")\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6fca0",
   "metadata": {},
   "source": [
    "# Create the DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_name = index_name + \"datasource\"\n",
    "print(\"Datasource name: \", datasource_name)\n",
    "BLOB_CONTAINER_NAME = \"docconvodocs\"\n",
    "print(\"Container name: \", BLOB_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd19b16",
   "metadata": {},
   "source": [
    "## Create the blob container that the datasource will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e3543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "connect_str = os.environ[\"AZURE_BLOB_STORAGE_CONNECTION_STRING\"]\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "\n",
    "def create_blob_container(blob_service_client: BlobServiceClient, container_name):\n",
    "    try:\n",
    "        container_client = blob_service_client.create_container(name=container_name)\n",
    "    except ResourceExistsError:\n",
    "        print(\"A container with this name already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe51f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_blob_container(blob_service_client, BLOB_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec8be6",
   "metadata": {},
   "source": [
    "## Creating datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afe47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\"connectionString\": os.environ[\"AZURE_BLOB_STORAGE_CONNECTION_STRING\"]},\n",
    "    \"dataDeletionDetectionPolicy\": {\n",
    "        \"@odata.type\": \"#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy\",\n",
    "        \"softDeleteColumnName\": \"IsDeleted\",\n",
    "        \"softDeleteMarkerValue\": \"true\",\n",
    "    },\n",
    "    \"container\": {\"name\": BLOB_CONTAINER_NAME},\n",
    "}\n",
    "r = requests.put(\n",
    "    os.environ[\"AZURE_SEARCH_ENDPOINT\"] + \"/datasources/\" + datasource_name,\n",
    "    data=json.dumps(datasource_payload),\n",
    "    headers=headers,\n",
    "    params=params,\n",
    ")\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b8489",
   "metadata": {},
   "source": [
    "# Create Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_name = index_name + \"indexer\"\n",
    "print(\"Indexer name: \", indexer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f833ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer\n",
    "indexer_payload = {\n",
    "    \"name\": indexer_name,\n",
    "    \"dataSourceName\": datasource_name,\n",
    "    \"targetIndexName\": index_name,\n",
    "    \"skillsetName\": skillset_name,\n",
    "    \"schedule\": {\"interval\": \"PT30M\"},  # How often do you want to check for new content in the data source\n",
    "    \"fieldMappings\": [\n",
    "        {\"sourceFieldName\": \"metadata_title\", \"targetFieldName\": \"title\"},\n",
    "        {\"sourceFieldName\": \"metadata_storage_name\", \"targetFieldName\": \"name\"},\n",
    "        {\"sourceFieldName\": \"metadata_storage_path\", \"targetFieldName\": \"location\"},\n",
    "    ],\n",
    "    \"outputFieldMappings\": [],\n",
    "    \"parameters\": {\n",
    "        \"maxFailedItems\": -1,\n",
    "        \"maxFailedItemsPerBatch\": -1,\n",
    "        \"configuration\": {\"dataToExtract\": \"contentAndMetadata\", \"imageAction\": \"generateNormalizedImages\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "r = requests.put(\n",
    "    os.environ[\"AZURE_SEARCH_ENDPOINT\"] + \"/indexers/\" + indexer_name,\n",
    "    data=json.dumps(indexer_payload),\n",
    "    headers=headers,\n",
    "    params=params,\n",
    ")\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "print(r.ok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcel-spike-dY-SZnE2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
